{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Block #1: first CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1767/1767 [==============================] - 648s 367ms/step - loss: 1.4501 - accuracy: 0.4270 - val_loss: 0.8116 - val_accuracy: 0.4631\n",
      "\n",
      "Epoch 00001: val_accuracy improved from inf to 0.46307, saving model to ./emotion_little_vgg_4.h5\n",
      "Epoch 2/50\n",
      "1767/1767 [==============================] - 602s 341ms/step - loss: 1.4228 - accuracy: 0.4353 - val_loss: 1.0886 - val_accuracy: 0.4642\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.46307\n",
      "Epoch 3/50\n",
      "1767/1767 [==============================] - 614s 347ms/step - loss: 1.3923 - accuracy: 0.4510 - val_loss: 1.0466 - val_accuracy: 0.4548\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.46307 to 0.45480, saving model to ./emotion_little_vgg_4.h5\n",
      "Epoch 4/50\n",
      "1767/1767 [==============================] - 616s 348ms/step - loss: 1.3816 - accuracy: 0.4569 - val_loss: 1.6996 - val_accuracy: 0.4937\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.45480\n",
      "Epoch 5/50\n",
      "1767/1767 [==============================] - 609s 345ms/step - loss: 1.3652 - accuracy: 0.4607 - val_loss: 0.8036 - val_accuracy: 0.4514\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.45480 to 0.45139, saving model to ./emotion_little_vgg_4.h5\n",
      "Epoch 6/50\n",
      "1767/1767 [==============================] - 614s 347ms/step - loss: 1.3547 - accuracy: 0.4682 - val_loss: 0.9435 - val_accuracy: 0.4827\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.45139\n",
      "Epoch 7/50\n",
      "1767/1767 [==============================] - 610s 345ms/step - loss: 1.3365 - accuracy: 0.4742 - val_loss: 0.7650 - val_accuracy: 0.4864\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.45139\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"./emotion_little_vgg_3.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[201  42  26  51 147  24]\n",
      " [106  64  33  66 136 123]\n",
      " [ 28  10 758  36  28  19]\n",
      " [105  27 160 105 138  91]\n",
      " [ 75  25  53 119 304  18]\n",
      " [ 18  25  29  16  10 318]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.38      0.41      0.39       491\n",
      "        Fear       0.33      0.12      0.18       528\n",
      "       Happy       0.72      0.86      0.78       879\n",
      "     Neutral       0.27      0.17      0.21       626\n",
      "         Sad       0.40      0.51      0.45       594\n",
      "    Surprise       0.54      0.76      0.63       416\n",
      "\n",
      "    accuracy                           0.50      3534\n",
      "   macro avg       0.44      0.47      0.44      3534\n",
      "weighted avg       0.46      0.50      0.46      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('./emotion_little_vgg_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"rajeev.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
